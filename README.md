# Recommender_Systems

## 算法篇 Algorithm

### FM

推荐系统一般面临的问题是评分预测问题，给定用户集合 $U = {u_1, u_2, u_3,...u_M}$、 物品集合 $I = {i_1, i_2, i_3,.....i_N}$ ,模型是一个评分函数：
$f: \mathbb{U} \times \mathbb{I} \rightarrow \mathbb{R}$

$y = f(u, i)$ 表示用户$u$对物品$i$的评分。

其中已知部分用户在物品上的评分：$\mathbb{S} \in \mathbb{U} \times \mathbb{I}, \forall(u, i) \in \mathbb{S}, \tilde{y}=f(u, i)$，目标是求解剩余用户再剩余物品上的评分：$\hat{y}=f(u, i), \forall(u, i) \in \complement_{\mathbb{S}}$

其中 $\mathrm{C}_{\mathbb{S}}$ 为 $\mathbb{S}$ 的补集。

- 通常评分问题是一个回归问题，模型预测结果是评分的大小。此时损失函数采用 $MAE/MSE$ 等等。

- 也可以将其作为一个分类问题，模型预测结果是各评级的概率。此时损失函数是交叉熵。

- 当评分只有 0 和 1 时，这表示用户对商品 “喜欢/不喜欢”，或者 “点击/不点击”。这就是典型的点击率预估问题。

LR模型是简单的线性模型，原理简单，易于理解，训练非常方便，对于一般的分类或者预测问题，可以很容易的训练。但是，LR模型特征之间是彼此独立的，无法拟合特征之间的非线性关系，而现实生活中的特征之间往往不是独立的而是存在一定的内在联系。以新闻推荐为例，一般男性用户看军事新闻多，而女性用户喜欢情感类新闻，那么可以看出性别与新闻的类别有一定的关联性，如果能找出这类相关的特征，是非常有意义的，可以显著提升模型预测的准确度。

#### 一、LR模型
其中 $X$ 属于n维特征向量, 即 $X \in \mathbb{R}^{n}, y$ 属于目标值，回归问题中 $y \in \mathbb{R}$, 二分类问题中 $y \in\{-1,+1\}$


$\hat{y}(X)=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}$

从LR模型可以看到：
>(1) 各个特征分量 $x_{i}$ 和 $x_{j}(i \neq j)$ 彼此之间是独立的
(2) $\hat{y}(X)$ 将单个特征分量线性的组合起来，却忽略了特征分量彼此之间的相互组合关系；

所以LR模型只考虑了一阶特征的线性组合关系

对于特征组合关系来说，我们定义如下：
>(1) 一阶特征：即单个特征，不产生新特征，如 $x_{1}$
(2) 二阶特征：即两个特征组合产生的新特征，如 $x_{1} x_{2}$
(3) 高阶特征：即两个以上的特征组合产生的新特征, 如 $x_{1} x_{2} x_{3}$


#### 二、多项式模型方程
为了克服模型二阶特征的组合因素，我们用LR模型改写为二阶多项式模型：

$\hat{y}(X)=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}$

其中 $x_{i} x_{j}$ 表示两个互异特征组合的二阶特征, $w_{i j}$ 表示二阶特征的交叉项系数
至此，该模型似乎已经加入了特征组合的因素，接下来只要学习参数即可

但是，上述二阶多项式模型却有一个致命的缺陷：
>数据稀疏性普遍存在的实际应用场景中，二阶特征系数 $w_{i j}$ 的训练是很困难的

造成的学习困难是因为：
>(1) $w_{i j}$ 的学习需要大量特征分量 $x_{i}$ 和 $x_{j}$ 都非零的样本
(2) 样本本身是稀流的，同时满足 $x_{i} x_{j} \neq 0$ 的样本非常稀少

所以多项式模型虽然加入了二阶特征组合，但是收到了数据稀疏的影响

#### 三、FM模型方程
为了克服模型无法在稀疏数据场景下学习二阶特征系数 $w_{i j}$, 我们需要将 $w_{i j}$ 表示为另外一种
形式
为此，针对样本 $X$ 的第维特征分量 $x_{i}$, 引入辅助隐向量 $v_{i}$
$$
v_{i}=\left(v_{i 1}, v_{i 2}, \ldots, v_{i k}\right)^{T} \in \mathbb{R}^{k}
$$

其中k是超参数，表示特征分量 $x_{i}$ 对应一个 $\mathrm{k}$ 维隐向量 $v_{i}$, 则将 $w_{i j}$ 表示为:
$$
w_{i j}=<v_{i}, v_{j}>=\sum_{f=1}^{k} v_{i f} v_{j f}
$$

上面引入隐向量的含义为：
>-阶特征系数 $w_{i j}$ 等价于：特征分量 $x_{i}$ 和 $x_{j}$ 对应的隐向量 $v_{i}$ 和 $v_{j}$ 的内积
$<v_{i}, v_{j}>$, 这就是FM模型的核心思想

#### 四、FM模型
我们将二阶多项式模型改写为FM模型
$$
\hat{y}(X)=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n}<v_{i}, v_{j}>x_{i} x_{j}
$$

FM(Factorization machines, 因子分解机) 是点击率预估场景中最常见的算法模型，FM的参数为$w_{0} \in \mathbb{R}, w \in \mathbb{R}^{n}, V \in \mathbb{R}^{n \times k}$


各个参数的含义为：
>(1) $w_{0} \in \mathbb{R}$ 表示FM模型的偏置
(2) $w \in \mathbb{R}^{n}$ 表示FM模型对一阶特征的建模
(3) $V \in \mathbb{R}^{n \times k}$ 表示FM模型对二阶特征的建模

参数的个数为： $1+n+n k$
复杂度为： $O\left(n^{2} k\right)$

#### 五、FM具体求解




### DeepFM

#### Tensorflow

#### Pytorch


## 工程篇 Engineering